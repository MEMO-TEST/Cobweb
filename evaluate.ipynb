{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate MinN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree, NearestNeighbors\n",
    "from data_preprocess.config import bank,credit,census,meps\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from data_preprocess.bank import bank_data\n",
    "from data_preprocess.credit import credit_data\n",
    "from data_preprocess.census import census_data\n",
    "from data_preprocess.meps import meps_data\n",
    "import pandas as pd\n",
    "import miniball\n",
    "\n",
    "dataset_names = ['bank', 'census', 'credit', 'meps']\n",
    "method_names = ['Cobweb_BT_random','Cobweb_GAN_BT_random', 'expga', 'LIMI']\n",
    "column_names = ['Cobweb_BT_random','Cobweb_GAN_BT_random', 'expga', 'LIMI']\n",
    "data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"meps\": meps}\n",
    "dataset_dict = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, 'meps':meps_data}\n",
    "\n",
    "# Get leaf nodes\n",
    "def get_leaf_nodes(tree, data, leaf_size):\n",
    "    leaf_nodes = []\n",
    "    for i in range(0, data.shape[0], leaf_size):\n",
    "        leaf_nodes.append(data[i:i+leaf_size])\n",
    "    \n",
    "    # Calculate the center and radius of each leaf node\n",
    "    centers = []\n",
    "    radii = []\n",
    "    for node in leaf_nodes:\n",
    "        center = np.mean(node, axis=0)\n",
    "        radius = np.max(np.linalg.norm(node - center, axis=1))\n",
    "        centers.append(center)\n",
    "        radii.append(radius)\n",
    "    \n",
    "    return np.array(centers), np.array(radii)\n",
    "\n",
    "# Merge adjacent leaf nodes\n",
    "def merge_nodes(centers, radii, merge_radius):\n",
    "    nbrs = NearestNeighbors(radius=merge_radius).fit(centers)\n",
    "    merged = np.zeros(len(centers), dtype=bool)\n",
    "    num_merged_nodes = 0\n",
    "    \n",
    "    for i in range(len(centers)):\n",
    "        if not merged[i]:\n",
    "            indices = nbrs.radius_neighbors([centers[i]], return_distance=False)[0]\n",
    "            merged[indices] = True\n",
    "            num_merged_nodes += 1\n",
    "            \n",
    "    return num_merged_nodes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = 'MLP'\n",
    "\n",
    "    merge_radius = 1# Sets the distance of the merged sphere\n",
    "\n",
    "    log = []\n",
    "    log_index = []\n",
    "    log_path = f'./result/RQ3/{model}_MinN.csv'\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        sens_list = data_config[dataset_name].sensitive_param\n",
    "        for sens_index in sens_list:\n",
    "            print('==========================================')\n",
    "            print(f'========{dataset_name}-{sens_index}-{model}=========')\n",
    "            final_node_counts = []\n",
    "            for method_name in method_names:\n",
    "                sens = sens_index if 'BF' in method_name else sens_index + 1\n",
    "                data_path = f'result/{method_name}/{dataset_name}{sens}_{model}_{method_name}.npy'\n",
    "\n",
    "                data = np.load(data_path)\n",
    "                data = data[np.random.choice(data.shape[0], min(10000, len(data)), replace=False)]\n",
    "\n",
    "                leaf_size = 40\n",
    "                ball_tree = BallTree(data, leaf_size=leaf_size)\n",
    "\n",
    "                # Get the center and radius of leaf nodes\n",
    "                centers, radii = get_leaf_nodes(ball_tree, data, leaf_size)\n",
    "\n",
    "                # Calculate the number of nodes after merging\n",
    "                final_node_count = merge_nodes(centers, radii, merge_radius)\n",
    "                print(method_name, np.round(np.mean(np.array(radii))), final_node_count)\n",
    "                final_node_counts.append(final_node_count)\n",
    "            log_index.append(f'{dataset_name} {sens_index}')\n",
    "            log.append(final_node_counts)\n",
    "    log = [[np.round(x, 3) for x in row] for row in log]\n",
    "    df = pd.DataFrame(log, columns=column_names, index=log_index)\n",
    "    df.to_csv(log_path, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "import os\n",
    "from data_preprocess.config import bank, census, credit, meps\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    method_names = [ 'expga', 'LIMI', 'Cobweb_BT_random', 'Cobweb_GAN_BT_random']\n",
    "    legend_names = ['Expga', 'LIMI', 'Cobweb', 'Cobweb GAN']\n",
    "    dataset_names = ['bank', 'census', 'credit', 'meps']\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"meps\": meps}\n",
    "    # Set the global font size\n",
    "    plt.rcParams.update({'font.size': 42})\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    gs = GridSpec(nrows=2, ncols=4, figure=fig)\n",
    "\n",
    "    count = 0\n",
    "    for dataset_name in dataset_names:\n",
    "        for sens_param_index in data_config[dataset_name].sensitive_param:\n",
    "            print(f'-------------{dataset_name}-{sens_param_index}---------------------------')\n",
    "            sens_name = data_config[dataset_name].sens_name[sens_param_index + 1]\n",
    "            # read data\n",
    "            data_list = []\n",
    "            for method_name in method_names:\n",
    "                sens = sens_param_index if 'BF' in method_name else sens_param_index + 1\n",
    "                data = np.load(f'./result/{method_name}/{dataset_name}{sens}_MLP_{method_name}.npy')\n",
    "                data = np.delete(data, [sens_param_index], axis = 1)\n",
    "                data = data[np.random.choice(data.shape[0], min(len(data),10000),replace=False)]\n",
    "                data_list.append(data)\n",
    "\n",
    "            # Delete sensitive attribute columns\n",
    "            total_data = np.vstack(data_list)\n",
    "\n",
    "            # Standardization\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(total_data)\n",
    "\n",
    "            tsen = TSNE(n_components=2,perplexity=50, n_iter=1000, random_state=0) \n",
    "            tsne_results = []\n",
    "            for data in data_list:\n",
    "                tsne_result = tsen.fit_transform(data)\n",
    "                tsne_results.append(tsne_result)\n",
    "            \n",
    "            # set seabon\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            sns.set_context(\"notebook\", rc={\"font.size\":14, \n",
    "                                \"axes.titlesize\":16, \n",
    "                                \"axes.labelsize\":14})\n",
    "            colors = sns.color_palette(\"Set2\")\n",
    "            sub_gs = GridSpecFromSubplotSpec(2, 2, subplot_spec=gs[count])\n",
    "\n",
    "            count += 1\n",
    "            main_ax = None  \n",
    "            for i, (tsne_result, method_name, color) in enumerate(zip(tsne_results, legend_names, colors)):\n",
    "                if main_ax is None:\n",
    "                    ax = fig.add_subplot(sub_gs[int(i/2), i % 2])\n",
    "                    main_ax = ax  \n",
    "                else:\n",
    "                    ax = fig.add_subplot(sub_gs[int(i/2), i % 2], sharex=main_ax, sharey=main_ax)\n",
    "                ax.scatter(tsne_result[:, 0], tsne_result[:, 1], label= method_name, color=color)\n",
    "            # ax.set_xlabel('D1')\n",
    "            # ax.set_ylabel('D2')\n",
    "            # add legend\n",
    "                ax.legend(loc='upper right')\n",
    "                main_ax.set_title(f'{dataset_name} - {sens_name}')\n",
    "\n",
    "\n",
    "    fig.suptitle('TSNE visualization')\n",
    "    # Automatically adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./result/RQ2TSNE/compare.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选真实数据并记录真实值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "from data_preprocess.bank import bank_data\n",
    "from data_preprocess.credit import credit_data\n",
    "from data_preprocess.census import census_data\n",
    "from data_preprocess.meps import meps_data\n",
    "from data_preprocess.config import bank,credit,census,meps\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dataset_dict = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, 'meps':meps_data}\n",
    "data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"meps\": meps}\n",
    "\n",
    "# 判断新数据是否为异常点\n",
    "def is_real_data(sample, iso_forest):\n",
    "    prediction = iso_forest.predict(sample.reshape(1, -1))\n",
    "    return prediction == 1  # 1 表示真实数据，-1 表示异常点\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_names = ['bank','census', 'credit', 'meps']\n",
    "    # dataset_names = ['meps']\n",
    "    method_names = ['Cobweb_BT_random','Cobweb_BT_nocluster_random','expga','LIMI']\n",
    "    column_names = ['Cobweb_BT_random', 'Cobweb_BT_nocluster_random','expga','LIMI']\n",
    "    # method_names = ['Cobweb_BT_random','Cobweb_BT_GR_random','Cobweb_LR_random','Cobweb_GAN_BT_random','Cobweb_GAN_BT_GR_random','Cobweb_GAN_LR_random', 'expga', 'LIMI']\n",
    "    # column_names = ['Cobweb_BT_random','Cobweb_BT_GR_random','Cobweb_LR_random','Cobweb_GAN_BT_random','Cobweb_GAN_BT_GR_random','Cobweb_GAN_LR_random', 'expga', 'LIMI']\n",
    "    # method_names = ['Cobweb_BT_random','Cobweb_GAN_BT_random', 'expga', 'LIMI']\n",
    "    # column_names = ['Cobweb_BT_random','Cobweb_GAN_BT_random', 'expga', 'LIMI']\n",
    "\n",
    "    model = \"MLP\"\n",
    "    new_data_lens = []\n",
    "    result_info = []\n",
    "    row_index = []\n",
    "    result_path = f'result/{model}_natural.csv'\n",
    "    lens_path = f'natural_IDS/{model}_NID.csv'\n",
    "    for dataset_name in dataset_names:\n",
    "        \n",
    "        real_data, Y, input_shape, nb_classes = dataset_dict[dataset_name]()\n",
    "        \n",
    "        if os.path.exists(f'model_info/iso/{dataset_name}.pkl'):\n",
    "            iso_forest = joblib.load(f'model_info/iso/{dataset_name}.pkl')\n",
    "        else:\n",
    "            print(\"training\")\n",
    "            iso_forest = IsolationForest(contamination='auto', random_state=42).fit(real_data)\n",
    "            joblib.dump(iso_forest, f'model_info/iso/{dataset_name}.pkl')\n",
    "            print('train over')\n",
    "        for sens_param_index in data_config[dataset_name].sensitive_param:\n",
    "            row_index.append(f'{dataset_name}_{sens_param_index}')\n",
    "            print(f'------------------------{dataset_name}-{sens_param_index}---------------------')\n",
    "            temp_result = []\n",
    "            temp_lens = []\n",
    "            for method_name in method_names:\n",
    "                if not os.path.exists(f'natural_IDS/{method_name}/'):\n",
    "                    os.makedirs(f'natural_IDS/{method_name}/')\n",
    "                sens = sens_param_index if 'BF' in method_name else sens_param_index + 1\n",
    "                data_path = f'result/{method_name}/{dataset_name}{sens}_{model}_{method_name}.npy'\n",
    "                data = np.load(data_path)\n",
    "                test_result = np.array(list(map(lambda row: is_real_data(row, iso_forest), data)))\n",
    "                suc_rate = np.sum(test_result)/len(data)\n",
    "                temp_result.append(suc_rate)\n",
    "\n",
    "                new_IDS = data[test_result.ravel()]\n",
    "                temp_lens.append(len(new_IDS))\n",
    "                np.save(f'natural_IDS/{method_name}/{dataset_name}{sens}_{model}_{method_name}.npy',new_IDS)\n",
    "\n",
    "            new_data_lens.append(temp_lens)\n",
    "            result_info.append(temp_result)\n",
    "            print(method_names)\n",
    "            print(temp_result)\n",
    "    result_info = [[np.round(x, 3) for x in row] for row in result_info]\n",
    "    df = pd.DataFrame(result_info, columns=column_names, index=row_index)\n",
    "    df.to_csv(result_path, index=True)\n",
    "    new_data_lens = pd.DataFrame(new_data_lens, columns=column_names, index = row_index)\n",
    "    new_data_lens.to_csv(lens_path, index=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
